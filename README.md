# Credit Score Classification: Random Forest & XGBoost

## ğŸ“Œ Overview
This repository contains a **credit score classification project** using **Random Forest and XGBoost**. The goal is to predict a customerâ€™s credit score category based on various financial and behavioral attributes. The project includes **data preprocessing, feature engineering, hyperparameter tuning, model evaluation, and visualization**.

## ğŸš€ Models Used
1. **Random Forest** (Tuned, 77% Accuracy)
2. **XGBoost** (Tuned, 80% Accuracy)

## ğŸ“‚ Project Structure
```
ğŸ“¦ Credit-Score-Classification
â”‚-- ğŸ“ data/               # Raw & processed datasets (not included in GitHub)
â”‚-- ğŸ“ notebooks/          # Jupyter Notebooks for exploration & model building
â”‚-- ğŸ“ models/             # Trained model files (if applicable)
â”‚-- ğŸ“ results/            # Model performance reports & visualizations
â”‚-- ğŸ“œ Model_Comparison_Results.json # Detailed documentation
â”‚-- ğŸ“œ README.md           # Project overview & instructions
â”‚-- ğŸ“œ requirements.txt    # Required libraries
â”‚-- ğŸ“œ train_model.py      # Python script for training the models
```

## ğŸ“Š Data Description
The dataset includes customer financial information such as **income, credit card usage, number of bank accounts, delayed payments, and credit inquiries**. The target variable is the **credit score category**:
- **0** â†’ Poor
- **1** â†’ Standard
- **2** â†’ Good

## ğŸ›  Installation & Setup
To run this project locally, follow these steps:

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/your-username/Credit-Score-Classification.git
cd Credit-Score-Classification
```

### 2ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Run the Model Training
```bash
python train_model.py
```

### 4ï¸âƒ£ Evaluate the Models
```bash
python evaluate_model.py
```

## ğŸ“ˆ Results & Model Performance
- **Random Forest Accuracy:** `77%`
- **XGBoost Accuracy:** `80%`
- **Cross-validation applied to prevent overfitting**
- **Feature importance analysis included**

| Model         | Accuracy | Precision | Recall | F1 Score |
|--------------|----------|------------|--------|-----------|
| Random Forest | 77%     | 0.76      | 0.77   | 0.77      |
| XGBoost      | 80%     | 0.80      | 0.80   | 0.80      |

## ğŸ“Œ Visual Summary
Here are some key visualizations from the project:

1ï¸âƒ£ **Feature Correlation Heatmap**  
2ï¸âƒ£ **Confusion Matrices for Both Models**  
3ï¸âƒ£ **Model Performance Comparison**  

ğŸ“Š ![Model Comparison](results/model_comparison.png)

## ğŸ’¡ Key Takeaways
- **XGBoost slightly outperforms Random Forest** in this dataset.
- **Hyperparameter tuning significantly improved accuracy.**
- **Overfitting was managed using cross-validation & regularization techniques.**
- **SMOTE was used to handle class imbalance.**

## ğŸ† Future Improvements
- Testing **deep learning models** (ANNs, CNNs for tabular data).
- Exploring **other feature engineering techniques**.
- Deploying the best model as an **API or web app**.

---

## ğŸ“ Author
Deepak Shrivas  
ğŸ“§ Email: dannyshrivas31@gmail.com 
ğŸ”— LinkedIn: Deepak Shrivas(https://www.linkedin.com/in/deepak-shrivas)
ğŸ™ GitHub: Danny0068(https://github.com/Danny0068)

---

## ğŸŒŸ Contribute & Support
If you found this project helpful, please â­ the repository and feel free to contribute by submitting a **Pull Request**! ğŸš€
